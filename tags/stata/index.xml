<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stata on Michaël Aklin</title>
    <link>/tags/stata/</link>
    <description>Recent content in Stata on Michaël Aklin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright> </copyright>
    <lastBuildDate>Sat, 15 Sep 2018 00:00:00 +0000</lastBuildDate><atom:link href="/tags/stata/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Type S and Type M errors in Stata: New .ado file</title>
      <link>/news/2018-09-15-stata/</link>
      <pubDate>Sat, 15 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/news/2018-09-15-stata/</guid>
      <description>P values are notoriously unreliable. We can quickly fell prey to statistically significant estimates. To overcome this, one approach has been to either abandon them altogether or require much harsher significance levels (Benjamin et al. 2018).
Another approach has been to use additional statistics to evaluate the reliability of our findings. If you want to stay in the hypothesis testing framework (and its coarse significant vs. non-significant results), then it is particularly important not to be mislead by a particular finding.</description>
    </item>
    
  </channel>
</rss>
